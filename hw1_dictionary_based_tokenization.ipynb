{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"hw1_dictionary_based_tokenization.ipynb","provenance":[{"file_id":"https://github.com/ekapolc/NLP_2020/blob/master/HW1/hw1_dictionary_based_tokenization.ipynb","timestamp":1578321429972}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"VLSqGgtWHK7r","colab_type":"text"},"source":["# HW1: Dictionary-based Tokenization \n"]},{"cell_type":"markdown","metadata":{"id":"zeCSxvRIHK7u","colab_type":"text"},"source":["In this exercise, you are to implement a dictionary-based word segmentation algorithm. There are two Python functions that you need to complete: \n","<br>\n","* maximal_matching\n","* backtrack"]},{"cell_type":"markdown","metadata":{"id":"6_pqCGFcHK7u","colab_type":"text"},"source":["### Create a toy dictionary to test the algorithm\n","\n","This is based on the example shown in the lecture. \n","You will tokenize the following text string: \"ไปหามเหสี!\"\n","The toy dictoionary provided in this exercise includes all the charaters, syllables, and words that appear that the text string."]},{"cell_type":"code","metadata":{"id":"_E4XEyAUHK7v","colab_type":"code","colab":{}},"source":["thai_vocab = [\"ไ\",\"ป\",\"ห\",\"า\",\"ม\",\"เ\",\"ห\",\"ส\",\"ี\",\"ไป\",\"หา\",\"หาม\",\"เห\",\"สี\",\"มเหสี\",\"!\"]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4IQUEXNHK7x","colab_type":"text"},"source":["### Maximal matching \n","Complete the maximal matching  function below to tokenize the input text\n"]},{"cell_type":"code","metadata":{"id":"FAGarbvhHK7y","colab_type":"code","colab":{}},"source":["from math import inf #infinity\n","def maximal_matching(c):\n","    #Initialize an empty 2D list\n","    d  =[[None]*len(c) for _ in range(len(c))]\n","    \n","    ####FILL CODE HERE####\n","    for i in range(len(c)) :\n","      for j in range(i, len(c)) :\n","        if i == 0 and j == 0 :\n","          d[i][j] = 1\n","        elif c[i : j + 1] in thai_vocab :\n","          if i == 0 :\n","            d[i][j] = 1\n","          else :\n","            min_value = inf\n","            for k in range(i) :\n","              if d[k][i - 1] < min_value :\n","                min_value = d[k][i - 1]\n","            d[i][j] = 1 + min_value\n","        else :\n","          d[i][j] = inf\n","    ######################\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdBEldMdHK71","colab_type":"text"},"source":["# Backtracking\n","Complete the backtracking function below to find the tokenzied words.\n","It should return a list containing a pair of the beginning position and the ending position of each word.\n","In this example, it should return: \n","<br>\n","[(0, 1),(2, 3),(4, 8),(9, 9)]\n","<br> \n","#### Each pair contains the position of each word as follows:\n","(0, 1) ไป\n","<br>\n","(2, 3) หา\n","<br>\n","(4, 8) มเหสี\n","<br>\n","(9, 9) !\n"]},{"cell_type":"code","metadata":{"id":"jH8ldQc-HK72","colab_type":"code","colab":{}},"source":["def backtrack(d):\n","    eow = len(d)-1 # End of Word position\n","    word_pos = [] # Word position\n","    ####FILL CODE HERE####\n","    while eow >= 0 :\n","      min_value = inf\n","      for i in range(eow + 1) :\n","        # print(i, d[i][eow])\n","        if d[i][eow] != None and d[i][eow] < min_value :\n","          min_value = d[i][eow]\n","          end = eow\n","          eow = i\n","      word_pos.append((eow, end))\n","      eow -= 1\n","    ######################\n","    word_pos.reverse()\n","    return word_pos\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YaOMhvDCHK73","colab_type":"text"},"source":["### Test your maximal matching algorithm on a toy dictionary\n","\n","Expected output:\n","\n","[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","<br>\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","<br>\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","<br>\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","<br>\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","<br>\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","<br>\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","<br>\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","<br>\n","[None, None, None, None, None, None, None, None, None, 4] !\n","<br>"]},{"cell_type":"code","metadata":{"id":"6GFLA7G7HK74","colab_type":"code","outputId":"1e395edf-8841-4727-ac8a-f122ce23a2c3","executionInfo":{"status":"ok","timestamp":1578362242742,"user_tz":-420,"elapsed":803,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["input_text = \"ไปหามเหสี!\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[1, 1, inf, inf, inf, inf, inf, inf, inf, inf] ไ\n","[None, 2, inf, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, 2, 2, 2, inf, inf, inf, inf, inf] ห\n","[None, None, None, 3, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, 3, inf, inf, inf, 3, inf] ม\n","[None, None, None, None, None, 3, 3, inf, inf, inf] เ\n","[None, None, None, None, None, None, 4, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, 4, 4, inf] ส\n","[None, None, None, None, None, None, None, None, 5, inf] ี\n","[None, None, None, None, None, None, None, None, None, 4] !\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hEzlCMIBHK76","colab_type":"text"},"source":["### Test your backtracking algorithm on a toy dictionary\n","Expected output:\n","<br>\n","ไป|หา|มเหสี|!"]},{"cell_type":"code","metadata":{"id":"KEbDcQubHK76","colab_type":"code","outputId":"ddbb8740-022d-4f56-de9f-9106dc46f0c3","executionInfo":{"status":"ok","timestamp":1578362246146,"user_tz":-420,"elapsed":1090,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def print_tokenized_text(d, input_text):\n","    tokenized_text=[]\n","    for pos in backtrack(d):\n","        #print(pos)\n","        tokenized_text.append(input_text[pos[0]:pos[1]+1])\n","\n","    print(\"|\".join(tokenized_text))\n","    \n","print_tokenized_text(out,input_text)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["ไป|หา|มเหสี|!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DABaMhCgHK78","colab_type":"text"},"source":["# Now try it on a real dictionary"]},{"cell_type":"markdown","metadata":{"id":"ipjikKEIHK79","colab_type":"text"},"source":["For UNIX-based OS users, the following cell will download a dictionary (it's just a list of thai words). Alternatively, you can download it from this link: https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"zQoJI5gxHK79","colab_type":"code","outputId":"6e2fe66a-0cb0-4d81-d49f-e400befd4495","executionInfo":{"status":"ok","timestamp":1578362251743,"user_tz":-420,"elapsed":2713,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["!wget https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2020-01-07 01:57:30--  https://raw.githubusercontent.com/PyThaiNLP/pythainlp/dev/pythainlp/corpus/words_th.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1516953 (1.4M) [text/plain]\n","Saving to: ‘words_th.txt.5’\n","\n","\rwords_th.txt.5        0%[                    ]       0  --.-KB/s               \rwords_th.txt.5      100%[===================>]   1.45M  --.-KB/s    in 0.05s   \n","\n","2020-01-07 01:57:30 (27.5 MB/s) - ‘words_th.txt.5’ saved [1516953/1516953]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nP63DJabHK7_","colab_type":"code","outputId":"2d27a65f-cb50-4274-bc60-909c39e54bac","executionInfo":{"status":"ok","timestamp":1578362254314,"user_tz":-420,"elapsed":808,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["with open(\"words_th.txt\",encoding='utf-8-sig') as f:\n","    thai_vocab = f.read().splitlines() \n","print(\"Vocab size:\", len(thai_vocab))\n","print(thai_vocab[:10])\n","#you can add more vocab to the dictionary \n","thai_vocab.extend([\"ๆ\",\"!\"])\n","thai_vocab.extend([\"ป\", \"ห\", \"ม\", \"ห\", \"ส\"])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Vocab size: 62069\n","['ก็', 'ก ข ไม่กระดิกหู', 'ก.', 'ก.ค.', 'ก.ต.', 'ก.ป.ส.', 'ก.พ.', 'ก.พ.ด.', 'ก.ม.', 'ก.ย']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v-kOJZebHK8B","colab_type":"text"},"source":["## The output of your maximal matching algorithm on a new dictionary\n","Expected output:\n","<br>\n","[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n","<br>\n","[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n","<br>\n","[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n","<br>\n","[None, None, None, inf, inf, inf, inf, inf, inf] า\n","<br>\n","[None, None, None, None, 2, inf, inf, inf, 2] ม\n","<br>\n","[None, None, None, None, None, inf, 3, inf, inf] เ\n","<br>\n","[None, None, None, None, None, None, inf, inf, inf] ห\n","<br>\n","[None, None, None, None, None, None, None, 4, 4] ส\n","<br>\n","[None, None, None, None, None, None, None, None, inf] ี"]},{"cell_type":"code","metadata":{"id":"9Tt6y8QQHK8D","colab_type":"code","outputId":"798afe9b-4cb2-475c-92b6-d9ba7de1f9be","executionInfo":{"status":"ok","timestamp":1578362259963,"user_tz":-420,"elapsed":811,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["input_text = \"ไปหามเหสี\"\n","out = maximal_matching(input_text)\n","for i in range(len(out)):\n","    print(out[i],input_text[i])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[1, 1, inf, 1, inf, inf, inf, inf, inf] ไ\n","[None, 2, inf, inf, inf, inf, inf, inf, inf] ป\n","[None, None, 2, 2, 2, inf, inf, inf, inf] ห\n","[None, None, None, inf, inf, inf, inf, inf, inf] า\n","[None, None, None, None, 2, inf, inf, inf, 2] ม\n","[None, None, None, None, None, inf, 3, inf, inf] เ\n","[None, None, None, None, None, None, inf, inf, inf] ห\n","[None, None, None, None, None, None, None, 4, 4] ส\n","[None, None, None, None, None, None, None, None, inf] ี\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FWCZt-i0HK8G","colab_type":"text"},"source":["### Expected tokenized text\n","ไปหา|มเหสี"]},{"cell_type":"code","metadata":{"id":"cKDxf6s3HK8H","colab_type":"code","outputId":"7a3528c7-c90d-4861-fdf1-20f36386ebbe","executionInfo":{"status":"ok","timestamp":1578362263523,"user_tz":-420,"elapsed":789,"user":{"displayName":"Nithiwut Wilainuch","photoUrl":"","userId":"11907089159635509680"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print_tokenized_text(out,input_text)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["ไปหา|มเหสี\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iT3QpeXJYj0H","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}